diff --git a/brian2/conftest.py b/brian2/conftest.py
index 5a97ac81..c36a40a7 100644
--- a/brian2/conftest.py
+++ b/brian2/conftest.py
@@ -6,7 +6,7 @@
 import numpy as np
 import pytest
 
-from brian2.devices import reinit_devices
+from brian2.devices import reinit_devices, get_device
 from brian2.units import ms
 from brian2.core.clocks import defaultclock
 from brian2.core.functions import Function, DEFAULT_FUNCTIONS
@@ -99,6 +99,9 @@ def pytest_runtest_makereport(item, call):
     outcome = yield
     rep = outcome.get_result()
     if rep.outcome == 'failed':
+        project_dir = get_device().project_dir
+        if project_dir is not None:
+            rep.sections.append(('Standalone project directory', f'{project_dir}'))
         reinit_devices()
         if not fail_for_not_implemented:
             exc_cause = getattr(call.excinfo.value, '__cause__', None)
diff --git a/brian2/core/preferences.py b/brian2/core/preferences.py
index 3663c276..40cc65a6 100644
--- a/brian2/core/preferences.py
+++ b/brian2/core/preferences.py
@@ -6,7 +6,7 @@
 import re
 import os
 from collections.abc import MutableMapping
-from io import BytesIO
+from io import StringIO
 
 from brian2.utils.stringtools import deindent, indent
 from brian2.units.fundamentalunits import have_same_dimensions, Quantity
@@ -457,7 +457,7 @@ def reset_to_defaults(self):
         '''
         Resets the parameters to their default values.
         '''
-        self.read_preference_file(BytesIO(self.defaults_as_file))
+        self.read_preference_file(StringIO(self.defaults_as_file))
 
     def register_preferences(self, prefbasename, prefbasedoc, **prefs):
         '''
diff --git a/brian2/devices/cpp_standalone/device.py b/brian2/devices/cpp_standalone/device.py
index 5791bb61..e7911f7e 100644
--- a/brian2/devices/cpp_standalone/device.py
+++ b/brian2/devices/cpp_standalone/device.py
@@ -13,6 +13,8 @@
 import numbers
 import tempfile
 from distutils import ccompiler
+import time
+import datetime
 
 import numpy as np
 
@@ -65,6 +67,21 @@
         neurons. Now, its value is ignored.
         '''
         ),
+    make_cmd_unix=BrianPreference(
+        default='make',
+        docs='''
+        The make command used to compile the standalone project. Defaults to the
+        standard GNU make commane "make".'''
+        ),
+    run_cmd_unix=BrianPreference(
+        default='./main',
+        validator=lambda val: isinstance(val, str) or isinstance(val, list),
+        docs='''
+        The command used to run the compiled standalone project. Defaults to executing
+        the compiled binary with "./main". Must be a single binary as string or a list
+        of command arguments (e.g. ["./binary", "--key", "value"]).
+        '''
+        ),
     extra_make_args_unix=BrianPreference(
         default=['-j'],
         docs='''
@@ -182,7 +199,13 @@ def __init__(self):
         self.code_lines = {'before_start': [],
                            'after_start': [],
                            'before_end': [],
-                           'after_end': []}
+                           'after_end': [],
+                           'before_run': [],
+                           'after_run': []}
+
+        self.timers = {'run_binary': None,
+                       'compile': {'clean':None,
+                                   'make': None}}
 
         self.clocks = set([])
 
@@ -977,9 +1000,14 @@ def compile_source(self, directory, compiler, debug, clean):
             else:
                 with std_silent(debug):
                     if clean:
+                        _start_timer = time.time()
                         os.system('make clean >/dev/null 2>&1')
+                        self.timers['compile']['clean'] = time.time() - _start_timer
+                    make_cmd = prefs.devices.cpp_standalone.make_cmd_unix
                     make_args = ' '.join(prefs.devices.cpp_standalone.extra_make_args_unix)
-                    x = os.system('make %s' % (make_args, ))
+                    _start_timer = time.time()
+                    x = os.system('%s %s' % (make_cmd, make_args, ))
+                    self.timers['compile']['make'] = time.time() - _start_timer
                     if x != 0:
                         error_message = ('Project compilation failed (error '
                                          'code: %u).') % x
@@ -1019,7 +1047,12 @@ def run(self, directory, with_output, run_args):
             if os.name == 'nt':
                 x = subprocess.call(['main'] + run_args, stdout=stdout)
             else:
-                x = subprocess.call(['./main'] + run_args, stdout=stdout)
+                run_cmd = prefs.devices.cpp_standalone.run_cmd_unix
+                if isinstance(run_cmd, str):
+                    run_cmd = [run_cmd]
+                _start_timer = time.time()
+                x = subprocess.call(run_cmd + run_args, stdout=stdout)
+                self.timers['run_binary'] = time.time() - _start_timer
             if stdout is not None:
                 stdout.close()
             if x:
@@ -1035,6 +1068,7 @@ def run(self, directory, with_output, run_args):
                 run_time, completed_fraction = last_run_info.split()
                 self._last_run_time = float(run_time)
                 self._last_run_completed_fraction = float(completed_fraction)
+        print("INFO _last_run_time = {} s".format(self._last_run_time))
 
         # Make sure that integration did not create NaN or very large values
         owners = [var.owner for var in self.arrays]
@@ -1362,6 +1396,14 @@ def network_run(self, net, duration, report=None, report_period=10*second,
         # We store this as an instance variable for later access by the
         # `code_object` method
         self.enable_profiling = profile
+
+        # Allow setting `profile` in the `set_device` call (used e.g. in brian2cuda
+        # SpeedTest configurations)
+        if 'profile' in self.build_options:
+            build_profile = self.build_options.pop('profile')
+            if build_profile:
+                self.enable_profiling = True
+
         all_objects = net.sorted_objects
         net._clocks = {obj.clock for obj in all_objects}
         t_end = net.t+duration
@@ -1487,10 +1529,12 @@ def network_run(self, net, duration, report=None, report_period=10*second,
             if clock not in all_clocks:
                 run_lines.append('{net.name}.add(&{clock.name}, NULL);'.format(clock=clock, net=net))
 
+        run_lines.extend(self.code_lines['before_run'])
         run_lines.append('{net.name}.run({duration!r}, {report_call}, {report_period!r});'.format(net=net,
                                                                                               duration=float(duration),
                                                                                               report_call=report_call,
                                                                                               report_period=float(report_period)))
+        run_lines.extend(self.code_lines['after_run'])
         self.main_queue.append(('run_network', (net, run_lines)))
 
         # Manually set the cache for the clocks, simulation scripts might
diff --git a/brian2/devices/cpp_standalone/templates/makefile b/brian2/devices/cpp_standalone/templates/makefile
index f3a9ca0f..671fd787 100644
--- a/brian2/devices/cpp_standalone/templates/makefile
+++ b/brian2/devices/cpp_standalone/templates/makefile
@@ -4,9 +4,8 @@ SRCS = {{source_files}}
 H_SRCS = {{header_files}}
 OBJS = ${SRCS:.cpp=.o}
 OBJS := ${OBJS:.c=.o}
-CC = @g++
 OPTIMISATIONS = {{ compiler_flags }}
-CFLAGS = -c -Wno-write-strings $(OPTIMISATIONS) -I. {{ openmp_pragma('compilation') }} {{ compiler_debug_flags }}
+CXXFLAGS = -c -Wno-write-strings $(OPTIMISATIONS) -I. {{ openmp_pragma('compilation') }} {{ compiler_debug_flags }}
 LFLAGS = {{ openmp_pragma('compilation') }} {{ linker_flags }} {{ linker_debug_flags }}
 DEPS = make.deps
 
@@ -15,17 +14,17 @@ all: $(PROGRAM)
 .PHONY: all clean
 
 $(PROGRAM): $(OBJS) $(DEPS) makefile
-	$(CC) $(OBJS) -o $(PROGRAM) $(LFLAGS)
+	$(CXX) $(OBJS) -o $(PROGRAM) $(LFLAGS)
 
 clean:
 	{{ rm_cmd }}
 
 make.deps: $(SRCS) $(H_SRCS)
-	$(CC) $(CFLAGS) -MM $(SRCS) > make.deps
+	$(CXX) $(CXXFLAGS) -MM $(SRCS) > make.deps
 	
 ifneq ($(wildcard $(DEPS)), )
 include $(DEPS)
 endif
 
 %.o : %.cpp makefile
-	$(CC) $(CFLAGS) $< -o $@
+	$(CXX) $(CXXFLAGS) $< -o $@
diff --git a/brian2/devices/cpp_standalone/templates/synapses_push_spikes.cpp b/brian2/devices/cpp_standalone/templates/synapses_push_spikes.cpp
index 99504814..928ab4bf 100644
--- a/brian2/devices/cpp_standalone/templates/synapses_push_spikes.cpp
+++ b/brian2/devices/cpp_standalone/templates/synapses_push_spikes.cpp
@@ -26,14 +26,4 @@
         {{owner.name}}.advance();
         {{owner.name}}.push({{_eventspace}}, {{_eventspace}}[_num{{eventspace_variable.name}}-1]);
     }
-
-    {% if profiled %}
-    // Profiling
-    {% if openmp_pragma('with_openmp') %}
-    const double _run_time = omp_get_wtime() -_start_time;
-    {% else %}
-    const double _run_time = (double)(std::clock() -_start_time)/CLOCKS_PER_SEC;
-    {% endif %}
-    {{codeobj_name}}_profiling_info += _run_time;
-    {% endif %}
 {% endblock %}
diff --git a/brian2/tests/features/base.py b/brian2/tests/features/base.py
index 03a81bfe..d39594c5 100644
--- a/brian2/tests/features/base.py
+++ b/brian2/tests/features/base.py
@@ -10,6 +10,8 @@
 import re
 
 from brian2.utils.stringtools import indent
+from brian2.core.base import BrianObjectException
+from brian2.utils.logger import std_silent, BrianLogger
 
 from collections import defaultdict
 
@@ -218,19 +220,26 @@ def after_run(self):
                             with_output=False)
     
     
-def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
-    tempfilename = tempfile.mktemp('exception')
-    if n is None:
-        init_args = ''
-    else:
-        init_args = str(n)
+def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second,
+            profile=False, return_lrcf=False):
+    exception_file = tempfile.NamedTemporaryFile(
+        prefix='brian_test_exception',
+        delete=False
+    )
+    exception_file.close()
+    tempfilename = exception_file.name
+    net_objects_file = tempfile.NamedTemporaryFile(
+        prefix='brian_test_network_objects', delete=False
+    )
+    net_objects_file.close()
+    tempfilename_net_obj = net_objects_file.name
     code_string = '''
 __file__ = '{fname}'
 import brian2
-from {config_module} import {config_name}
+import {config_module}
 from {feature_module} import {feature_name}
-configuration = {config_name}()
-feature = {feature_name}({init_args})
+feature = {feature_name}({n})
+configuration = {config_module}.{config_name}(feature_test=feature, profile={profile})
 import warnings, traceback, pickle, sys, os, time
 warnings.simplefilter('ignore')
 try:
@@ -248,41 +257,108 @@ def results(configuration, feature, n=None, maximum_run_time=1e7*brian2.second):
             pass
     lrcf = configuration.get_last_run_completed_fraction()
     run_time = run_time/lrcf
-    prof_info = brian2.magic_network.profiling_info
     new_prof_info = []
-    for n, t in prof_info:
-        new_prof_info.append((n, t/lrcf))
+    try:
+        prof_info = brian2.magic_network.profiling_info
+        for n, t in prof_info:
+            new_prof_info.append((n, t/lrcf))
+    except ValueError:
+        pass
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((None, results, run_time, new_prof_info), f, -1)
+    pickle.dump((None, results, run_time, new_prof_info, lrcf), f, -1)
     f.close()
-except Exception, ex:
+except Exception as ex:
     #traceback.print_exc(file=sys.stdout)
     tb = traceback.format_exc()
     f = open(r'{tempfname}', 'wb')
-    pickle.dump((tb, ex, 0.0, []), f, -1)
+    try:
+        pickle.dump((tb, ex, 0.0, [], 0.0), f, -1)
+    except pickle.PicklingError:
+        print(tb)
+        raise
     f.close()
     '''.format(config_module=configuration.__module__,
                config_name=configuration.__name__,
                feature_module=feature.__module__,
                feature_name=feature.__name__,
                tempfname=tempfilename,
+               tempfname_net_obj=tempfilename_net_obj,
                fname=__file__,
-               init_args=init_args,
+               n=n if n is not None else "",
                maximum_run_time=float(maximum_run_time),
+               profile=profile,
                )
     args = [sys.executable, '-c',
             code_string]
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # checkout the commit specified in the DynamicConfigCreator
+        configuration.git_checkout()
+        # checkout the original version of the module defining the feature
+        configuration.git_checkout_feature(feature.__module__)
+        configuration.git_checkout_feature(configuration.__module__)
     # Run the example in a new process and make sure that stdout gets
     # redirected into the capture plugin
     p = subprocess.Popen(args, stdout=subprocess.PIPE,
-                         stderr=subprocess.PIPE)
+                         stderr=subprocess.PIPE,
+                         encoding='UTF-8')
     stdout, stderr = p.communicate()
-    #sys.stdout.write(stdout)
-    #sys.stderr.write(stderr)
+    if p.returncode:
+        sys.stdout.write(stdout)
+        sys.stderr.write(stderr)
     with open(tempfilename, 'rb') as f:
-        tb, res, runtime, profiling_info = pickle.load(f)
-    return tb, res, runtime, profiling_info
-    
+        tb, res, runtime, profiling_info, lrcf = pickle.load(f)
+    os.remove(tempfilename)
+    if isinstance(res, Exception):
+        tb = stdout + '\n' + stderr + '\n' + tb
+    else:
+        tb = stdout + '\n' + stderr
+    # Load additional benchmarking times
+    standalone_benchmarks = {}
+    benchmark_file = configuration.get_benchmark_file(
+        feature_name=feature.__name__, n=n
+    )
+    if os.path.exists(benchmark_file):
+        with open(benchmark_file, "r") as f:
+            for line in f.readlines():
+                name, time = line.split()
+                if time == "None":
+                    time = None
+                else:
+                    # We record in microseconds, convert to seconds
+                    time = float(time) / 10e6
+                standalone_benchmarks[name] = time
+    else:
+        print(f"ERROR, benchmark_file not found at {benchmark_file}")
+
+    python_benchmarks = {}
+    python_benchmark_file = configuration.get_python_benchmark_file(
+        feature_name=feature.__name__, n=n
+    )
+    if os.path.exists(python_benchmark_file):
+        with open(python_benchmark_file, "r") as f:
+            for line in f.readlines():
+                name, time = line.split()
+                if time == "None":
+                    time = None
+                else:
+                    time = float(time)
+                python_benchmarks[name] = time
+    else:
+        print(f"ERROR, python_benchmark_file not found at {python_benchmark_file}")
+
+    if hasattr(configuration, 'git_commit') and configuration.git_commit is not None:
+        # reset the current changes before checking out original commit
+        configuration.git_reset()
+        # check out the original commit
+        configuration.git_checkout(reverse=True)
+
+    return_vars = (
+        tb, res, runtime, profiling_info, python_benchmarks, standalone_benchmarks
+    )
+    if return_lrcf:
+        return_vars = (*return_vars, lrcf)
+    return return_vars
+
 
 def check_or_compare(feature, res, baseline, maxrelerr):
     feature = feature()
@@ -325,7 +401,7 @@ def run_feature_tests(configurations=None, feature_tests=None,
             txt = 'OK'
             sym = '.'
             exc = None
-            tb, res, runtime, prof_info = results(configuration, ft, maximum_run_time=maximum_run_time)
+            tb, res, runtime, prof_info, *_ = results(configuration, ft, maximum_run_time=maximum_run_time)
             if isinstance(res, Exception):
                 if isinstance(res, NotImplementedError):
                     sym = 'N'
@@ -469,7 +545,8 @@ def __str__(self):
 
 
 def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbose=True,
-                    n_slice=slice(None), maximum_run_time=1e7*brian2.second):
+                    n_slice=slice(None), maximum_run_time=1e7*brian2.second,
+                    profile=False, mark_not_completed=False):
     if configurations is None:
         # some configurations to attempt to import
         try:
@@ -485,17 +562,41 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
         print('Configurations:', ', '.join(c.name for c in configurations))
 
     full_results = {}
-    tag_results = defaultdict(lambda:defaultdict(list))
+    traceback = {}
+    brian_stdouts = {}
+    brian_stderrs = {}
+    feature_results = {}
+    benchmark_times = {}
     for ft in speed_tests:
         if verbose:
             print(ft.fullname()+': ', end=' ')
+            sys.stdout.flush()
         for n in ft.n_range[n_slice]:
             if verbose:
                 print('n=%d [' % n, end=' ')
+                sys.stdout.flush()
             for configuration in configurations:
                 sym = '.'
+                brian_stdout = ''
+                brian_stderr = ''
                 for _ in range(1+int(run_twice)):
-                    tb, res, runtime, prof_info = results(configuration, ft, n, maximum_run_time=maximum_run_time)
+                    if mark_not_completed:
+                        tb, res, runtime, prof_info, python_bench, standalone_bench, lrcf = results(
+                            configuration,
+                            ft,
+                            n,
+                            maximum_run_time=maximum_run_time,
+                            profile=profile,
+                            return_lrcf=mark_not_completed
+                        )
+                    else:
+                        tb, res, runtime, prof_info, python_bench, standalone_bench = results(
+                            configuration,
+                            ft,
+                            n,
+                            maximum_run_time=maximum_run_time,
+                            profile=profile,
+                        )
                 if isinstance(res, Exception):
                     if isinstance(res, NotImplementedError):
                         sym = 'N'
@@ -504,8 +605,31 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                     if configuration is DefaultConfiguration:
                         raise res
                     runtime = numpy.NAN
+                    stdout_file = None
+                    stderr_file = None
+                    if hasattr(std_silent, 'dest_fname_stdout'):
+                        stdout_file=std_silent.dest_fname_stdout
+                        stderr_file=std_silent.dest_fname_stderr
+                    if stdout_file is not None and os.path.exists(stdout_file):
+                        with open(stdout_file, 'r') as sfile:
+                            brian_stdout = sfile.read()
+                    else:
+                        brian_stdout = 'no stdout file found, cwd = {}'.format(stdout_file)
+                    if stderr_file is not None and os.path.exists(stderr_file):
+                        with open(stderr_file, 'r') as sfile:
+                            brian_stderr = sfile.read()
+                    else:
+                        brian_stderr = 'no stderr file found, cwd = {}'.format(stderr_file)
                 sys.stdout.write(sym)
+                sys.stdout.flush()
                 full_results[configuration.name, ft.fullname(), n, 'All'] = runtime
+                if mark_not_completed:
+                    # save last run completed fraction
+                    full_results[configuration.name, ft.fullname(), n, 'lrcf'] = lrcf
+                traceback[configuration.name, ft.fullname(), n] = tb
+                brian_stdouts[configuration.name, ft.fullname(), n] = brian_stdout
+                brian_stderrs[configuration.name, ft.fullname(), n] = brian_stderr
+                feature_results[configuration.name, ft.fullname(), n] = res
                 suffixtime = defaultdict(float)
                 overheadstime = float(runtime)
                 for codeobjname, proftime in prof_info:
@@ -518,20 +642,126 @@ def run_speed_tests(configurations=None, speed_tests=None, run_twice=True, verbo
                 for suffix, proftime in list(suffixtime.items()):
                     full_results[configuration.name, ft.fullname(), n, suffix] = proftime
                 full_results[configuration.name, ft.fullname(), n, 'Overheads'] = overheadstime
+                # Add additional benchmark timings
+                all_measures = list(standalone_bench.keys())
+                # Complicated hackmack to extract difference (after - before) for all
+                # measured times that have a before_ and after_ measurment. And leaving
+                # all other measurements as is.
+                before_after_measures = []
+                before_measures = []
+                after_measures = []
+                other_measuers = list(standalone_bench.keys())
+                for key in standalone_bench.keys():
+                    if key.startswith("before_"):
+                        truncated_key = key[len("before_"):]
+                        if truncated_key in after_measures:
+                            key_idx = before_measures.index(truncated_key)
+                            del after_measures[key_idx]
+                            before_after_measures.append(truncated_key)
+                        else:
+                            before_measures.append(truncated_key)
+                    elif key.startswith("after_"):
+                        truncated_key = key[len("after_"):]
+                        if truncated_key in before_measures:
+                            key_idx = before_measures.index(truncated_key)
+                            del before_measures[key_idx]
+                            before_after_measures.append(truncated_key)
+                        else:
+                            after_measures.append(truncated_key)
+                for measure in before_after_measures:
+                    before_key_idx = other_measuers.index(f"before_{measure}")
+                    del other_measuers[before_key_idx]
+                    after_key_idx = other_measuers.index(f"after_{measure}")
+                    del other_measuers[after_key_idx]
+                for measure in after_measures:
+                    assert f"after_{measure}" in other_measuers
+                for measure in before_measures:
+                    assert f"before_{measure}" in other_measuers
+                del before_measures
+                del after_measures
+                # Store (after - before) for all that have it
+                for measure in before_after_measures:
+                    before = standalone_bench[f"before_{measure}"]
+                    after = standalone_bench[f"after_{measure}"]
+                    if after is None and before is None:
+                        #print(
+                        #    f"WARNING: Can't get standalone benchmark point "
+                        #    f"'{measure}' timepoint is None for {configuration.name}"
+                        #)
+                        pass
+                    else:
+                        time = after - before
+                        full_results[configuration.name, ft.fullname(), n, f"standalone_{measure}"] = time
+                # Store time points for all tha don't have after and before
+                for measure in other_measuers:
+                    if measure is None:
+                        #print(
+                        #    f"WARNING: Can't get benchmark point '{measure}', "
+                        #    f"timepoint is None for {configuration.name}"
+                        #)
+                        pass
+                    else:
+                        time = standalone_bench[measure]
+                        full_results[configuration.name, ft.fullname(), n, f"{measure}"] = time
+                # Store some additional differences
+                diffs = [
+                    ("before_start", "after_end"),
+                    ("before_start", "before_run"),
+                    ("after_run", "after_end")
+                ]
+                for (before_key, after_key) in diffs:
+                    if before_key in all_measures and after_key in all_measures:
+                        measure = f"standalone_{before_key}_to_{after_key}"
+                        before = standalone_bench[before_key]
+                        after = standalone_bench[after_key]
+                        time = after - before
+                        full_results[configuration.name, ft.fullname(), n, measure] = time
+                    else:
+                        #print(f"WARNING: {before_key} or {after_key} not found.")
+                        pass
+                # Store timings taken from Python
+                for measure, time in python_bench.items():
+                    if time is None:
+                        #print(
+                        #    f"WARNING: Can't get Python benchmark point '{measure}', "
+                        #    f"timepoint is None for {configuration.name}"
+                        #)
+                        pass
+                    else:
+                        full_results[configuration.name, ft.fullname(), n, f"python_{measure}"] = time
+
             if verbose:
                 print(']', end=' ')
+                sys.stdout.flush()
         if verbose:
             print()
-        
-    return SpeedTestResults(full_results, configurations, speed_tests)
+            for n in ft.n_range[n_slice]:
+                for conf in configurations:
+                    if isinstance(feature_results[conf.name, ft.fullname(), n], Exception):
+                        print(f"\nTRACEBACK {conf.name} N={n}"
+                              f"\n{traceback[conf.name, ft.fullname(), n]}"
+                              f"\nSTDOUT"
+                              f"\n{brian_stdouts[conf.name, ft.fullname(), n]}"
+                              f"\nSTDERR"
+                              f"\n{brian_stderrs[conf.name, ft.fullname(), n]}"
+                              f"\n\n")
+
+    return SpeedTestResults(full_results, feature_results, configurations, speed_tests,
+                            brian_stdouts, brian_stderrs, traceback)
 
 
 class SpeedTestResults(object):
-    def __init__(self, full_results, configurations, speed_tests):
+    def __init__(self, full_results, feature_results, configurations, speed_tests,
+                 brian_stdouts, brian_stderrs, tracebacks):
         self.full_results = full_results
+        # Either the exception of the run or the run's `n`
+        self.feature_results = feature_results
         self.configurations = configurations
         self.speed_tests = speed_tests
-        
+        self.brian_stdouts = brian_stdouts
+        self.brian_stderrs = brian_stderrs
+        self.tracebacks = tracebacks
+
     def get_ns(self, fullname):
         L = [(cn, fn, n, s)
              for cn, fn, n, s in self.full_results
@@ -539,14 +769,51 @@ def get_ns(self, fullname):
         confignames, fullnames, n, codeobjsuffixes  = zip(*L)
         return numpy.array(sorted(list(set(n))))
 
-    def get_codeobjsuffixes(self, fullname):
+    def get_codeobjsuffixes(self, fullname, exclude=("python_", "standalone_"),
+                            only=""):
+        """
+        Exclude suffixes starting with anything in `exclude` (list of strings) or only
+        choose those starting with `only` (single string)
+        """
         L = [(cn, fn, n, s)
              for cn, fn, n, s in self.full_results
              if fn == fullname]
         confignames, fullnames, n, codeobjsuffixes  = zip(*L)
-        return set(codeobjsuffixes)
-
-    def plot_all_tests(self, relative=False, profiling_minimum=1.0):
+        if exclude and only:
+            for ex in exclude:
+                if not ex.startswith(only):
+                    raise ValueError(
+                        f"Can't exclude ({exclude}) suffixes with only ({only}) suffix"
+                    )
+        if not isinstance(only, str):
+            raise TypeError(f"`only` has to be string, got {type(only)}")
+        if not isinstance(exclude, tuple):
+            raise TypeError(f"`exclude` has to be tuple, got {type(exclude)}")
+        filtered_codeobjectsuffixes = []
+        # Add these always to the filtered suffixes
+        for suffix in ["All", "Overheads", "lrcf"]:
+            if suffix in codeobjsuffixes:
+                filtered_codeobjectsuffixes.append(suffix)
+        for suffix in codeobjsuffixes:
+            # Only add this suffix if it doesn't start with any exclude string
+            exclude_append = True
+            if exclude:
+                exclude_append = False
+                if not suffix.startswith(exclude):
+                    exclude_append = True
+            # Only add this suffix if it starts with only (if given)
+            only_append = True
+            if only:
+                only_append = False
+                if suffix.startswith(only):
+                    only_append = True
+            if exclude_append and only_append:
+                filtered_codeobjectsuffixes.append(suffix)
+        return set(filtered_codeobjectsuffixes)
+
+    def plot_all_tests(self, relative=False, profiling_minimum=1.0,
+                       print_relative=False, exclude=("python_", "standalone_"),
+                       only=""):
         if relative and profiling_minimum<1:
             raise ValueError("Cannot use relative plots with profiling")
         import pylab
@@ -554,10 +821,17 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             fullname = st.fullname()
             pylab.figure()
             ns = self.get_ns(fullname)
-            codeobjsuffixes = self.get_codeobjsuffixes(fullname)
+            codeobjsuffixes = self.get_codeobjsuffixes(
+                fullname, exclude=exclude, only=only
+            )
             codeobjsuffixes.remove('All')
             codeobjsuffixes.remove('Overheads')
-            codeobjsuffixes = ['All', 'Overheads']+sorted(codeobjsuffixes)
+            if 'lrcf' in codeobjsuffixes:
+                codeobjsuffixes.remove('lrcf')
+            if only:
+                codeobjsuffixes = sorted(codeobjsuffixes)
+            else:
+                codeobjsuffixes = ['All', 'Overheads'] + sorted(codeobjsuffixes)
             if relative or profiling_minimum==1:
                 codeobjsuffixes = ['All']
             baseline = None
@@ -566,31 +840,46 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
             dashes = {}
             markerstyles = {}
             for isuffix, suffix in enumerate(codeobjsuffixes):
-                cols = itertools.cycle(pylab.rcParams['axes.color_cycle'])
-                for (iconfig, config), col in zip(enumerate(self.configurations), cols):
+                props = itertools.cycle(pylab.rcParams['axes.prop_cycle'])
+                for (iconfig, config), prop in zip(enumerate(self.configurations), props):
                     configname = config.name
                     runtimes = []
+                    not_finished = []
                     skip = True
                     for n in ns:
                         runtime = self.full_results.get((configname, fullname, n, 'All'), numpy.nan)
+                        if 'lrcf' in codeobjsuffixes:
+                            lrcf = self.full_results.get((configname, fullname, n, 'lrcf'), numpy.nan)
+                            not_finished.append(lrcf != 1.0)
+                        else:
+                            not_finished = [0]  # no plotting
                         thistime = self.full_results.get((configname, fullname, n, suffix), numpy.nan)
                         if float(thistime/runtime)>=profiling_minimum:
                             skip = False
                         runtimes.append(thistime)
+                        #overheadstime = self.full_results.get((configname, fullname, n, 'Overheads'), numpy.nan)
+                        #if (profiling_minimum<1 and  overheadstime == runtime:
+                        #    skip = True
                     if skip:
                         continue
                     runtimes = numpy.array(runtimes)
-                    if relative:
+                    if relative or print_relative:
                         if baseline is None:
                             baseline = runtimes
+                    if relative:
                         runtimes = baseline/runtimes
+                    if print_relative:
+                        rel = baseline/runtimes
+                        for ni, n in enumerate(ns):
+                            print("INFO relative performance for {ft} N={n} {conf}: {factor}".format(
+                                ft=fullname, n=n, conf=config.name, factor=rel[ni]))
                     if suffix=='All':
                         lw = 2
                         label = configname
                     else:
                         lw = 1
                         label = suffix
-                    plottable = sum(-numpy.isnan(runtimes[1:]+runtimes[:-1]))
+                    plottable = sum(~numpy.isnan(runtimes[1:]+runtimes[:-1]))
                     if plottable:
                         if label in havelabel:
                             label = None
@@ -612,8 +901,12 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                                         dash = dash+(4, 2)
                                 dashes[suffix] = dash
                                 markerstyles[suffix] = msty = next(markerstyles_cycle)
-                        line = pylab.plot(ns, runtimes, lw=lw, color=col, marker=msty,
+                        line = pylab.plot(ns, runtimes, lw=lw, color=prop['color'], marker=msty,
                                           mec='none', ms=8, label=label)[0]
+                        if suffix == 'All' and sum(not_finished) != 0:
+                            pylab.plot(ns[not_finished], runtimes[not_finished],
+                                       linestyle='None', marker=r'$\circlearrowleft$',
+                                       ms=15, color=prop['color'], label='linear runtime interpolation')
                         if dash is not None:
                             line.set_dashes(dash)
             pylab.title(fullname)
@@ -623,6 +916,7 @@ def plot_all_tests(self, relative=False, profiling_minimum=1.0):
                 pylab.gca().set_xscale('log')
             if st.time_axis_log:
                 pylab.gca().set_yscale('log')
+            pylab.grid(True, which='both')
 
 # Code below auto generates restructured text tables, copied from:
 # http://stackoverflow.com/questions/11347505/what-are-some-approaches-to-outputting-a-python-data-structure-to-restructuredte
diff --git a/brian2/tests/features/speed.py b/brian2/tests/features/speed.py
index 317bbf6d..0b6ba2f6 100644
--- a/brian2/tests/features/speed.py
+++ b/brian2/tests/features/speed.py
@@ -23,7 +23,7 @@ class LinearNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Linear 1D"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000, 1000000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 261015625]  #fail: 262031250
     n_label = 'Num neurons'
 
     # configuration options
@@ -42,7 +42,7 @@ class HHNeuronsOnly(SpeedTest):
     category = "Neurons only"
     name = "Hodgkin-Huxley"
     tags = ["Neurons"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 102750000]  #fail: 103125000
     n_label = 'Num neurons'
 
     # configuration options
@@ -86,7 +86,7 @@ class CUBAFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "CUBA fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000, 3546875]  #fail: 3562500
     n_label = 'Num neurons'
 
     # configuration options
@@ -132,7 +132,7 @@ class COBAHHFixedConnectivity(SpeedTest):
     category = "Full examples"
     name = "COBAHH fixed connectivity"
     tags = ["Neurons", "Synapses", "SpikeMonitor"]
-    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]
+    n_range = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 3781250]  #fail: 3812500
     n_label = 'Num neurons'
 
     # configuration options
@@ -255,7 +255,7 @@ def run(self):
 class SynapsesOnly(object):
     category = "Synapses only"
     tags = ["Synapses"]
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 1000000]
     n_label = 'Num neurons'
     duration = 1 * second
     # memory usage will be approximately p**2*rate*dt*N**2*bytes_per_synapse/1024**3 GB
@@ -282,7 +282,7 @@ class VerySparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Very sparse, medium rate (10s duration)"
     rate = 10 * Hz
     p = 0.02
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -290,21 +290,21 @@ class SparseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, medium rate (1s duration)"
     rate = 10 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 1234375]  #fail: 1242187  # weave max CPU time should be about 5m
 
 
 class DenseMediumRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Dense, medium rate (1s duration)"
     rate = 10 * Hz
     p = 1.0
-    n_range = [10, 100, 1000, 10000, 40000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 546875]  #fail: 554687  # weave max CPU time should be about 4m
 
 
 class SparseLowRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, low rate (10s duration)"
     rate = 1 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000, 100000]
+    n_range = [10, 100, 1000, 10000, 100000, 500000, 1000000, 3875000]  #fail: 3906250  # weave max CPU time should be about 20s
     duration = 10 * second
 
 
@@ -312,7 +312,7 @@ class SparseHighRateSynapsesOnly(SynapsesOnly, SpeedTest):
     name = "Sparse, high rate (1s duration)"
     rate = 100 * Hz
     p = 0.2
-    n_range = [10, 100, 1000, 10000]
+    n_range = [10, 100, 1000, 10000, 100000, 387500]  #fail: 393750  # weave max CPU time should be about 5m
 
 
 if __name__ == '__main__':
diff --git a/brian2/tests/test_preferences.py b/brian2/tests/test_preferences.py
index 1c2941f9..9ba7e72f 100644
--- a/brian2/tests/test_preferences.py
+++ b/brian2/tests/test_preferences.py
@@ -169,6 +169,9 @@ def test_brianglobalpreferences():
     # check that load_preferences works, but nothing about its values
     gp = BrianGlobalPreferences()
     gp.load_preferences()
+    # Check that resetting to default preferences works
+    gp = BrianGlobalPreferences()
+    gp.reset_to_defaults()
 
 
 @pytest.mark.codegen_independent
